{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ee2ced4",
   "metadata": {
    "id": "9ee2ced4"
   },
   "source": [
    "# Lecture 10 & 11 -- Classification\n",
    "In this lecture, students will learn about classification analysis and how to estimate classification models using the `scikit-learn` package."
   ]
  },
  {
   "cell_type": "code",
   "id": "d455e19e",
   "metadata": {
    "id": "d455e19e"
   },
   "source": [
    "# Lecture Modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model, model_selection, metrics\n",
    "from sklearn import tree, ensemble\n",
    "import warnings\n",
    "from IPython.display import display\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e524014f",
   "metadata": {
    "id": "e524014f"
   },
   "source": [
    "## What is Classification Analysis?\n",
    "Classificiation, like regression, entails prediction of a target variable given some set of input variables. The difference is that classification is focused on predicting a **label** or **class** whereas regression is concerned with predicting a continuous target variable. For example, instead of predicting the sale price of a house given its characteristics, we could predict whether or not the sale price was above or over 500,000 dollars. Other general examples include determining\n",
    "- the subject of a photo\n",
    "- whether a credit card transaction is fraudulent\n",
    "- the genre of music of a particular song\n",
    "- the political leaning of a newspaper article\n",
    "\n",
    "## Binary Classification\n",
    "\n",
    "**Binary clasisfication** refers to a classifications problem that has only two possible labels. All of these problems could be framed as binary classification problems.\n",
    "- A house price is either above or below 500,000 dollars\n",
    "- A photo either contains a stop sign or not (image captcha)\n",
    "- A credit card transaction is either fraudulent or not fraudulent\n",
    "- A song is either classical or not.\n",
    "- A newspaper article is either left or right leaning\n",
    "\n",
    "## Multi-class Classification\n",
    "**Multi-class Classification** entails predicting a label that can take on more than two values. All of the previous general problems could also be framed as multi-class classification problems.\n",
    "- A house price can be in one of the three buckets:\n",
    "    - less than 350,000 dollars,\n",
    "    - between 350,000 and 500,00 dollars, or\n",
    "    - over 500,000 dollars.\n",
    "- A photo of a flower can be classified as one of over 100 different types of flowers\n",
    "- A credit car transaction could be classified as:\n",
    "    - Fraudulent\n",
    "    - Not Fraudulent\n",
    "    - Need Attention from Fraud Speciailist\n",
    "- A song could be classified as one of many genres\n",
    "- A newspaper article can be either:\n",
    "    - far left\n",
    "    - left\n",
    "    - center\n",
    "    - right\n",
    "    - far right\n",
    "    - apolitical\n",
    "\n",
    "## Classification Algorithms\n",
    "To begin, we will learn about various classification algorithms in the context of binary classification and extend them to multi-class problems later.\n",
    "\n",
    "Fortunately, both linear regresison and our tree-based algorithms have classification analog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05848b06",
   "metadata": {
    "id": "05848b06"
   },
   "source": [
    "## Logistic Regression\n",
    "**Logistic regression** entails modeling the probability of an event taking place as a *transformed* linear function of many independent variables. For example, we can model the probability that the price of particular house is greater than 500,000 dollars as\n",
    "\n",
    "$$\n",
    "Pr(price_i > 500,000) = L\\left(\\beta_0 + \\beta_1 * sqft\\_living_i +  \\beta_2 * bathrooms_i + \\epsilon \\right)\n",
    "$$\n",
    "\n",
    "where $L$ is the eponymous Logistic function defined as\n",
    "$$\n",
    "L(x) := \\frac{1}{1 + e^{-x}}\n",
    "$$.\n",
    "\n",
    "Let's plot the logistic function below."
   ]
  },
  {
   "cell_type": "code",
   "id": "0dd88988",
   "metadata": {
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "0dd88988",
    "outputId": "f47616b0-991b-4ef1-b12a-d20a385e3660"
   },
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "y = 1/(1+np.exp(-x))\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Logistic Function\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ba4d263",
   "metadata": {
    "id": "8ba4d263"
   },
   "source": [
    "## Useful Characteristics of Logistic Regression\n",
    "This function is nice for binary clasisfication:\n",
    "- Range of Logistic function is $(0,1)$\n",
    "- Domain of Logistic function  is $(-\\infty, \\infty)$\n",
    "\n",
    "The range is important because\n",
    "- We are estimating probabilities which have to be between 0 and 1\n",
    "- By giving us the probability of one label (e.g. $Pr(price > 500,000) = p$), we automatically get the probability of the other label (e.g. $Pr(price \\leq 500,000) = 1-p$)\n",
    "\n",
    "The domain simply allows the function can take any number as an input.\n",
    "\n",
    "**Understanding Check**\n",
    "- For what value of $x$ is $L(X)$ equal to $0$?\n",
    "- What happens to $L(x)$ when $x$ moves to the right?\n",
    "- What happens to $L(x)$ when $x$ moves to the left?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa8b0d",
   "metadata": {
    "id": "2ffa8b0d"
   },
   "source": [
    "## Example: Recidivism\n",
    "Recidivism refers to the tendency of convicted criminals to commit crimes after being released from prison. High recidivism rates likely reflect flaws in the criminal justice system and society at large.\n",
    "\n",
    "Recidivism, like all crime, is costly to the victims of the crime and also expends judicial, law enforcement, and correctional facility resources.   As a result, there is a lot of interest in ascertaining what types of policies and prison programs can help prevent inmates from reoffending when they are released. These include everything from efforts to destigmatize former criminals to job training programs for inmates.\n",
    "\n",
    "Being able to predict whether or not a criminal will reoffend at the time of sentencing might also useful. If a criminal is less likely of reoffend, a judge might find a lighter sentence to be more appropriate.  As you might be able to guess, this gets some into some ethically dubious territory, especially if factors such as an individual's race are used as inputs to that prediction algorithm.\n",
    "\n",
    "Before sentencing, some courts in the United States use a tool called COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) to predict the probability of a given criminal reoffending. [This article](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) found that COMPAS was horribly biased against black defendants and over-predicted their probability of recidivism among other serious issue.\n",
    "\n",
    "Below, we download data used in the study. originating from the Broward County (Florida) Sheriff's office. It contains data on defendants, their characteristics, previous criminal records, whether or not they reoffended in the two years after their conviction, and their COMPAS score.  Let's take a look at the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "44d991a2",
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44d991a2",
    "outputId": "d137727c-3bbc-4b9c-ea27-fc73531e864c"
   },
   "source": [
    "data_url = \"https://raw.githubusercontent.com/propublica/compas-analysis\"\n",
    "data_url += \"/master/compas-scores-two-years.csv\"\n",
    "\n",
    "df = pd.read_csv(data_url)\n",
    "\n",
    "df.info()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "qgbGq0YAEHHR",
    "outputId": "38d1d53d-d841-4755-d2fc-ca65209fb00e"
   },
   "id": "qgbGq0YAEHHR",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "43b8bd55",
   "metadata": {
    "id": "43b8bd55"
   },
   "source": [
    "First, we are going to observe how well the COMPAS score predicts whether a given individual reoffended within two years of being scored using logistic regression.\n",
    "\n",
    "The COMPAS prediction is captured by the `decile_score` variable which is on a scale from 1 to 10. A higher number means an individual has a greater risk of recidivism.\n",
    "\n",
    "`two_year_recid` is our dependent variable that is equal to 1 if a given individual reoffended within two years and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define X and Y\n",
    "X = df[[\"decile_score\"]]\n",
    "y = df[\"two_year_recid\"]"
   ],
   "metadata": {
    "id": "2oSDqwTTEQMX"
   },
   "id": "2oSDqwTTEQMX",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Split data\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.25, random_state=123)"
   ],
   "metadata": {
    "id": "KHQzo3veEQHz"
   },
   "id": "KHQzo3veEQHz",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Initialize logistic regression (don't worry about what the solver argument does)\n",
    "logistic_model = linear_model.LogisticRegression(solver=\"lbfgs\", penalty = None)\n",
    "\n",
    "# Fit model on training data\n",
    "logistic_model.fit(X_train, y_train)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "k2rBmt1vEP9l",
    "outputId": "a45593ac-7777-49e3-a855-066a80487e6a"
   },
   "id": "k2rBmt1vEP9l",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7486a8bd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7486a8bd",
    "outputId": "909e73a5-ab50-42bc-ff00-10bf8967ddc6"
   },
   "source": [
    "# Get Coeffs\n",
    "beta_0 = logistic_model.intercept_[0]\n",
    "beta_1 = logistic_model.coef_[0][0]\n",
    "\n",
    "print(f\"Fit model: Pr(recid) = L({beta_0:.4f} + {beta_1:.4f} decile_score)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b750f078",
   "metadata": {
    "id": "b750f078"
   },
   "source": [
    "## Interpretation\n",
    "What does a positive coefficient say about the relationship between `decile_score` and the probability of two-year recidivism? Given what this score is being used for, does that make sense?\n",
    "\n",
    "## Time to Classify\n",
    "The Logistic Regression alone does not tell us whether or not someone will reoffend within two years. To do this, we need to establish a **threhold** or **cutoff** value on the probabilities we estimate. For example, if we choose a cutoff of $Pr(recid) = .5$, we have the following classification scheme:\n",
    "\n",
    "$$\n",
    "\\hat{recid} = \\left\\{\\begin{array}{cc} 1 & \\text{if } Pr(recid) > .5 \\\\ 0 & \\text{if } Pr(recid)  \\leq .5\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "where 1 indicates that the individual will reoffend. In this model, person with a decile score such that $-1.3705 + 0.2623 \\text{ decile_score} > 0$ will be classified as reoffender. In fact with some algebraic manipulation, we can see this happens whenver somebody has a decile score of 6 or more. Make sure you understand why this is.\n",
    "\n",
    "\n",
    "While .5 seems like a reasonable cutoff, it is actually arbitrary. Such a cutoff might not actually result in the \"best\" predictions. Before adjusting this cutoff boundary and talking about what \"best\" means, let's add another independent variable!"
   ]
  },
  {
   "cell_type": "code",
   "id": "14e9b439",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14e9b439",
    "outputId": "1890f258-93d0-4db5-8da4-736eb03a7137"
   },
   "source": [
    "# Redefine X\n",
    "X = df[[\"decile_score\", \"age\"]]\n",
    "\n",
    "# Split into training and testing sample\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Fit logistic regression model\n",
    "logistic_age_model = linear_model.LogisticRegression(solver=\"lbfgs\", penalty = None)\n",
    "logistic_age_model.fit(X_train, y_train)\n",
    "\n",
    "beta_0 = logistic_age_model.intercept_[0]\n",
    "beta_1, beta_2 = logistic_age_model.coef_[0]\n",
    "\n",
    "print(f\"Fit model: Pr(recid) = L({beta_0:.4f} + {beta_1:.4f} decile_score + {beta_2:.4f} age)\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6ba8f1c",
   "metadata": {
    "id": "b6ba8f1c"
   },
   "source": [
    "## Prediction\n",
    "Below, we can see two of `logistic_age_model`'s prediction methods. One predicts the label based on some cutoff and the other predicts the probabilities for each label.\n",
    "\n",
    "To predict labels, `sklearn` enforces a default cutoff as .5."
   ]
  },
  {
   "cell_type": "code",
   "id": "d7a5d17d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7a5d17d",
    "outputId": "56bd5e9f-37da-48bd-c1c5-8b2581666678"
   },
   "source": [
    "print(logistic_age_model.predict(X_train.iloc[15:25, :]))\n",
    "print(logistic_age_model.predict_proba(X_train.iloc[15:25, :]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3fec23d4",
   "metadata": {
    "id": "3fec23d4"
   },
   "source": [
    "## Interpretation\n",
    "Decile Score has a very similar relationship with the probability of recidivism whereas age seems to be slightly negatively correlated with the changes of recidivism.\n",
    "\n",
    "## Visualization\n",
    "We are going to visualize the problem now. Will consist of two parts\n",
    "- A scatter plot on `decile_score`/`age` axes indicating whether an individual reoffendes (blue dots) or not (orange dots) within two years\n",
    "- A contour plot that is orange for the values of  `decile_score`/`age` that result in a prediction of reoffense\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "81bc3e6c",
   "metadata": {
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "81bc3e6c",
    "outputId": "43ad7932-e5bd-451a-a9dc-bf922b84c32c"
   },
   "source": [
    "# Intialize Figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Scatter Plot\n",
    "ax.set_xlabel(\"decile_score\")\n",
    "ax.set_ylabel(\"age\")\n",
    "\n",
    "# Make 1d grids\n",
    "x1_grid = np.linspace(0, 10, 1000)\n",
    "x2_grid = np.linspace(0, 100, 1000)\n",
    "\n",
    "# Make 2d grid out of 1d grids\n",
    "x1, x2 = np.meshgrid(x1_grid, x2_grid)\n",
    "# Predict on these grids (catch warnings about X names)\n",
    "with warnings.catch_warnings(record=True):\n",
    "    Z = logistic_age_model.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "Z = Z.reshape(x1.shape)\n",
    "im = ax.contourf(x1, x2, Z, colors = [\"orange\", \"blue\"], levels = 1)\n",
    "fig.colorbar(im, ax=ax, ticks=np.unique(Z))\n",
    "scatter_colors = [\"gold\" if value == 0 else \"deepskyblue\" for value in y_train ]\n",
    "X_train.plot.scatter(x=\"decile_score\", y=\"age\", c = scatter_colors, ax=ax)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a47d5c27",
   "metadata": {
    "id": "a47d5c27"
   },
   "source": [
    "## Logistic Regression Geometry\n",
    "It is clear that our logistic regression is not predicting everything perfectly. If it were, we would only see blue dots in the blue region and orange dots in the orange region.\n",
    "\n",
    "Note how our **decision boundary** (the border of the blue and orange regions) is a line in this two-dimensional coordinate space.\n",
    "\n",
    "This means logistic regression can only perfectly classify the data when the labels are **linearly separable**. In two-dimensions, this means they can be separated by a line. In three-dimensions, a plane. The more dimensions you have, the \"easier\" it is to separate data.\n",
    "\n",
    "## Geometry of Cutoff Value\n",
    "Decreasing the cutoff value shifts the line left. Increasing it shifts the line right. The slope does **not** change. To change the cutoff value, we use `.predict_proba()` and compare its second column to the cutoff value."
   ]
  },
  {
   "cell_type": "code",
   "id": "75507813",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "75507813",
    "outputId": "33f38297-4271-4622-eef8-00188d9c6de0"
   },
   "source": [
    "# Redefine X\n",
    "X = df[[\"decile_score\", \"age\"]]\n",
    "\n",
    "# Split into training and testing sample\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Fit logistic regression model\n",
    "logistic_age_model = linear_model.LogisticRegression(solver=\"lbfgs\", penalty = None)\n",
    "logistic_age_model.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e02838d9",
   "metadata": {
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "e02838d9",
    "outputId": "7e1f1108-e2a4-43a7-a53c-43096b8dd24f"
   },
   "source": [
    "cutoff = .4\n",
    "# Intialize Figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Scatter Plot\n",
    "\n",
    "ax.set_xlabel(\"decile_score\")\n",
    "ax.set_ylabel(\"age\")\n",
    "\n",
    "# Make 1d grids\n",
    "x1_grid = np.linspace(0, 10, 1000)\n",
    "x2_grid = np.linspace(0, 100, 1000)\n",
    "\n",
    "# Make 2d grid out of 1d grids\n",
    "x1, x2 = np.meshgrid(x1_grid, x2_grid)\n",
    "# Predict on these grids (catch warnings about X names)\n",
    "with warnings.catch_warnings(record=True):\n",
    "    Z = logistic_age_model.predict_proba(np.c_[x1.ravel(), x2.ravel()])[:,1] >= cutoff\n",
    "Z = Z.reshape(x1.shape)\n",
    "im = ax.contourf(x1, x2, Z, colors = [\"orange\", \"blue\"], levels = 1)\n",
    "fig.colorbar(im, ax=ax, ticks=np.unique(Z))\n",
    "scatter_colors = [\"gold\" if value == 0 else \"deepskyblue\" for value in y_train ]\n",
    "X_train.plot.scatter(x=\"decile_score\", y=\"age\", c = scatter_colors, ax=ax)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73c717f1",
   "metadata": {
    "id": "73c717f1"
   },
   "source": [
    "## Polynomials\n",
    "We can add polynomial terms (`age_sq`, `decile_sq`) as we did with linear regression and interaction terms (`age_decile`) to make these lines more flexible."
   ]
  },
  {
   "cell_type": "code",
   "id": "69dc31ee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "69dc31ee",
    "outputId": "a16441e8-9a35-4535-9f1d-13dc6c534aa9"
   },
   "source": [
    "# Redefine X\n",
    "X_sq = df[[\"decile_score\", \"age\"]]\n",
    "with warnings.catch_warnings(record=True):\n",
    "    X_sq[\"decile_sq\"] = X_sq[\"decile_score\"] ** 2\n",
    "    X_sq[\"age_sq\"] = X_sq[\"age\"] ** 2\n",
    "    X_sq[\"age_decile\"] = X_sq[\"decile_score\"] * X_sq[\"age\"]\n",
    "\n",
    "# Split into training and testing sample\n",
    "X_sq_train, X_sq_test, y_sq_train, y_sq_test = model_selection.train_test_split(\n",
    "    X_sq, y, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Fit logistic regression model\n",
    "logistic_age_model_sq = linear_model.LogisticRegression(solver=\"lbfgs\", penalty = None)\n",
    "logistic_age_model_sq.fit(X_sq_train, y_sq_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "10d91059",
   "metadata": {
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 492
    },
    "id": "10d91059",
    "outputId": "a34ed0dd-d129-46f9-8c1a-fb6a73d49538"
   },
   "source": [
    "# Intialize Figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Scatter Plot\n",
    "ax.set_xlabel(\"decile_score\")\n",
    "ax.set_ylabel(\"age\")\n",
    "\n",
    "# Make 1d grids\n",
    "x1_grid = np.linspace(0, 10, 1000)\n",
    "x2_grid = np.linspace(0, 100, 1000)\n",
    "\n",
    "x1, x2 = np.meshgrid(x1_grid, x2_grid)\n",
    "with warnings.catch_warnings(record=True):\n",
    "    Z = logistic_age_model_sq.predict(np.c_[x1.ravel(), x2.ravel(), x1.ravel() ** 2, x2.ravel() ** 2, x1.ravel() * x2.ravel()])\n",
    "Z = Z.reshape(x1.shape)\n",
    "im = ax.contourf(x1, x2, Z, colors = [\"orange\", \"blue\"], levels = 1)\n",
    "fig.colorbar(im, ax=ax, ticks=np.unique(Z))\n",
    "scatter_colors = [\"gold\" if value == 0 else \"deepskyblue\" for value in y_train ]\n",
    "X_train.plot.scatter(x=\"decile_score\", y=\"age\", c = scatter_colors, ax=ax)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d1e0052c",
   "metadata": {
    "id": "d1e0052c"
   },
   "source": [
    "## Model Evaluation\n",
    "In general, choosing different cutoffs amounts to choosing different classification models. This is also true for models that have different independent variables or coefficient estimates.\n",
    "\n",
    "As a result, we want some way to evaluate these models both in-sample and out-of-sample using various metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ce86fb",
   "metadata": {
    "id": "b0ce86fb"
   },
   "source": [
    "## Accuracy\n",
    "The most obvious measure of fit for a classification model is **accuracy**. This is the fraction of correct prediction over total predictions. We can use the `.score()` method to get the accuracy of our models. They are ultimately very similar (which should not surprise you given their geometrical overlap). Since the test accuracies and trainign accuracies are so similar, we are likely underfitting with both models.\n",
    "\n",
    "A good visual example of how accuracy works is given by a confussion amtrix. A good intro can be found [here](https://www.evidentlyai.com/classification-metrics/confusion-matrix)"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca9c4b20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ca9c4b20",
    "outputId": "8a40f34a-ef21-4558-8734-c1cc3be0e979"
   },
   "source": [
    "# Just linear terms\n",
    "train_acc = logistic_age_model.score(X_train, y_train)\n",
    "test_acc = logistic_age_model.score(X_test, y_test)\n",
    "\n",
    "# Quadratic and interaction terms\n",
    "train_acc_sq = logistic_age_model_sq.score(X_sq_train, y_sq_train)\n",
    "test_acc_sq = logistic_age_model_sq.score(X_sq_test, y_sq_test)\n",
    "\n",
    "\n",
    "print(\"Model with linear terms\")\n",
    "print(f\"Training Accuracy: {train_acc*100:.2f}% , Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(\"\")\n",
    "print(\"Model with quadratic and interaction terms\")\n",
    "print(f\"Training Accuracy: {train_acc_sq*100:.2f}% , Test Accuracy: {test_acc_sq*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe015c90",
   "metadata": {
    "id": "fe015c90"
   },
   "source": [
    "## Precision & Recall\n",
    "Sometimes Accuracy is not enough, especially in the face of **class imbalance** -- when you have more (or significantly more) of one label than another. For example, we might want to classify whether or not a person has a very rare disease. If it appears in .001% of the population, a model that always predicts false would have a 99.999% accuracy rate, but would not catch the disease when a patient actually needs an intervention. In this case, we need to make a distinction between the types of predictions our model can make:\n",
    "- true positives (predict the disease and they have it)\n",
    "- true negatives (predict no disease and they do not have it)\n",
    "- false positives (predict the disease and they do not have it)\n",
    "- false negatives (predict no disease and they do have it)\n",
    "\n",
    "Accuracy does not distniguish between true positives and true negatives nor false positives and false negatives, but these types of errors can have very different real life consequences. To account for such nuance, there are two popular metrics:\n",
    "- **Precision** is equal to porpotion of true positives out of the total number of positive predictions. Answers the question: when the model predicted true, how often was it right?\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "- **Recall** is equal to the number of true positives over the number of actual positives. It tries to answer, what proportion of actual positives was identified correctly?\n",
    "$$\n",
    "Precision = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Here is an [intuitive guide](https://developers.google.com/machine-learning/crash-course/classification/precision-and-recall)\n",
    "\n",
    "The model we described that always guesses negative always has zero true positives. Consequently, it always has 0 precision and 0 recall.\n",
    "\n",
    "When you want to consider both metrics, the **F1 Score** serves as a useful composite metric.\n",
    "$$F1 = 2 \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}$$\n",
    "SciKit-learn has a built in functionality to calculate these numbers for us. We demonstrate"
   ]
  },
  {
   "cell_type": "code",
   "id": "70dd3537",
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70dd3537",
    "outputId": "d140f89c-ab76-47a3-c82a-004d986dea41"
   },
   "source": [
    "print(\"Model with Linear Terms -- In-sample\")\n",
    "report = metrics.classification_report(\n",
    "    y_train, logistic_age_model.predict(X_train),\n",
    "    target_names=[\"no recidivism\", \"recidivism\"]\n",
    ")\n",
    "print(report)\n",
    "\n",
    "print(\"\")\n",
    "print(\"Model with Linear Terms -- Out-of-sample\")\n",
    "report = metrics.classification_report(\n",
    "    y_test, logistic_age_model.predict(X_test),\n",
    "    target_names=[\"no recidivism\", \"recidivism\"]\n",
    ")\n",
    "print(report)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "16d5ffd6",
   "metadata": {
    "id": "16d5ffd6"
   },
   "source": [
    "## Precision-Recall Tradeoff\n",
    "Frequently, increasing precision comes at a cost of decreasing recall. This tradeoff is inherent when adjusting our threshold value for fixed coefficient values.\n",
    "\n",
    "As the threshold gets lower, the algorithm is more likely to guess true. This can result in two types of reclassifications:\n",
    "\n",
    "- False negative to True Positive\n",
    "- True negative to False Positive\n",
    "\n",
    "In the first case, **precision and recall increase**. In the second case, **recall stays the same** and **precision decreases**. It is good to be aware of this tradeoff. If you want to learn more, look at QuantEcon's explanation of the **ROC curve** and **AUC cross-validation** [here](https://datascience.quantecon.org/tools/classification.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8e110b",
   "metadata": {
    "id": "ff8e110b"
   },
   "source": [
    "## Classification Trees & Random Forest Classifiers\n",
    "With some minor modifications, we can use the tree algorithms from the regression section for classification as well. We will jump right into multiclass prediction as it is functionally similar to binary label prediction.\n",
    "\n",
    "## Classification Trees\n",
    "Like regresison trees, classification trees are grown by iteratively splitting up our data. Unlike regression trees, classification trees need to predict labels. Since we are not dealing with ordered numeric values (e.g price, age, etc.) aymore we need to move away from mean-squared error (MSE) in favor of multi-label classification metrics.\n",
    "\n",
    "A common choice for label prediction is to take the mode (the most common) label within a given node. With that as a mapping, we will look at using two metrics to replace MSE as our criteria (loss) function when growing the tree:\n",
    "- gini impurity\n",
    "- entropy\n",
    "\n",
    "We will discuss all of these momentarily. First, let's use a decision tree to try and predict recidivism and plot its contours plot. We will use various max_depths and see which one performs best out of sample."
   ]
  },
  {
   "cell_type": "code",
   "id": "a3718b56",
   "metadata": {
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 523
    },
    "id": "a3718b56",
    "outputId": "de061728-0008-4ec8-db8c-df5c21cd1a62"
   },
   "source": [
    "# Initalize List of Decision Tree\n",
    "trees = []\n",
    "depths = [3, 5, 7, 9, 11, 13]\n",
    "# Append fit regression trees to list\n",
    "for depth in depths:\n",
    "    trees.append(tree.DecisionTreeClassifier(max_depth = depth).fit(X_train, y_train))\n",
    "\n",
    "\n",
    "# Plot\n",
    "# Make 1d grids\n",
    "x1_grid = np.linspace(0, 10, 1000)\n",
    "x2_grid = np.linspace(0, 100, 1000)\n",
    "\n",
    "# Make 2d grid out of 1d grids\n",
    "x1, x2 = np.meshgrid(x1_grid, x2_grid)\n",
    "\n",
    "\n",
    "# Intialize Figure\n",
    "plt.figure(figsize=(20,20))\n",
    "fig, axes = plt.subplots(2,3)\n",
    "for (i,ax) in enumerate(axes.flat):\n",
    "    ax.set_title(f\"Tree of Depth {depths[i]}\")\n",
    "    ax.set_xlabel(\"decile_score\")\n",
    "    ax.set_ylabel(\"age\")\n",
    "    with warnings.catch_warnings(record=True):\n",
    "        Z = trees[i].predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "    Z = Z.reshape(x1.shape)\n",
    "    im = ax.contourf(x1, x2, Z, colors = [\"orange\", \"blue\"], levels = 1)\n",
    "    fig.colorbar(im, ax=ax, ticks=np.unique(Z))\n",
    "    scatter_colors = [\"gold\" if value == 0 else \"deepskyblue\" for value in y_train ]\n",
    "    X_train.plot.scatter(x=\"decile_score\", y=\"age\", c = scatter_colors, ax=ax)\n",
    "fig.tight_layout()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7ea96687",
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ea96687",
    "outputId": "b0926d04-7da0-4202-ed50-66aceb6cb0f6"
   },
   "source": [
    "for (i, tm) in enumerate(trees):\n",
    "    print(f\"Training Accuracy for Tree of Depth {depths[i]} is {tm.score(X_train, y_train): .4f}\")\n",
    "    print(f\"Testing Accuracy for Tree of Depth {depths[i]} is {tm.score(X_test, y_test): .4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fcae5340",
   "metadata": {
    "id": "fcae5340"
   },
   "source": [
    "## Tree Visualization\n",
    "Let's visualize the tree of depth 3."
   ]
  },
  {
   "cell_type": "code",
   "id": "cc4075c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cc4075c2",
    "outputId": "a208197f-3a57-4aac-cca5-5dc80ca747ac"
   },
   "source": [
    "plt.figure(figsize=(120,120))\n",
    "tree.plot_tree(trees[0], max_depth = 4);"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f66992a",
   "metadata": {
    "id": "5f66992a",
    "outputId": "10540a4b-f615-40e1-da17-cfce969f03a3"
   },
   "source": [
    "df[\"priors_count\"].max()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f3e01c5",
   "metadata": {
    "id": "1f3e01c5"
   },
   "source": [
    "## Multiclass Labels\n",
    "We can also do this with multi class labels. Now we try and predict the `decile_score` using `age`"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Create sex indicator variable\n",
    "df[\"sex_indicator\"] = [1 if s == \"Female\" else 0 for s in df.sex]\n",
    "\n",
    "# Define X and Y\n",
    "X_m = df[[\"priors_count\", \"age\", \"juv_fel_count\",\n",
    "        \"juv_misd_count\", \"juv_other_count\",\n",
    "        \"is_violent_recid\", \"sex_indicator\"]]\n",
    "\n",
    "y_m = df[\"decile_score\"]\n",
    "\n",
    "# Split data\n",
    "X_m_train, X_m_test, y_m_train, y_m_test = model_selection.train_test_split(X_m, y_m, test_size=0.25)"
   ],
   "metadata": {
    "id": "3ooRpbjWMEQa"
   },
   "id": "3ooRpbjWMEQa",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3f493c2c",
   "metadata": {
    "scrolled": false,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "3f493c2c",
    "outputId": "eceb07ad-55f2-4088-a3e8-3f8b5c2f3da4"
   },
   "source": [
    "X_mt = df[[\"priors_count\", \"age\"]]\n",
    "y_mt = df[\"decile_score\"]\n",
    "\n",
    "# Split data\n",
    "X_mt_train, X_mt_test, y_mt_train, y_mt_test = model_selection.train_test_split(X_mt, y_mt, test_size=0.25)\n",
    "\n",
    "\n",
    "# Initialize and Fit Tree\n",
    "multi_tree = tree.DecisionTreeClassifier(max_depth = 5).fit(X_mt_train, y_mt_train)\n",
    "\n",
    "# Plot\n",
    "# Make 1d grids\n",
    "x1_grid = np.linspace(0, 40, 1000)\n",
    "x2_grid = np.linspace(0, 100, 1000)\n",
    "\n",
    "# Make 2d grid out of 1d grids\n",
    "x1, x2 = np.meshgrid(x1_grid, x2_grid)\n",
    "\n",
    "\n",
    "# Plot Boundary\n",
    "plt.figure(figsize=(20,20))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(f\"Tree of Depth 5\")\n",
    "ax.set_xlabel(\"priors_count\")\n",
    "ax.set_ylabel(\"age\")\n",
    "with warnings.catch_warnings(record=True):\n",
    "    Z = multi_tree.predict(np.c_[x1.ravel(), x2.ravel()])\n",
    "Z = Z.reshape(x1.shape)\n",
    "im = ax.contourf(x1, x2, Z, levels = 11)\n",
    "fig.colorbar(im, ax=ax, ticks=np.unique(Z))\n",
    "\n",
    "X_mt_train.plot.scatter(x=\"priors_count\", y=\"age\", c = y_m_train, ax=ax, cmap = \"viridis\")\n",
    "fig.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f5897d4f",
   "metadata": {
    "id": "f5897d4f"
   },
   "source": [
    "We can quickly examine in-sample and out-of-sample accuracy"
   ]
  },
  {
   "cell_type": "code",
   "id": "d1d4c9b8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1d4c9b8",
    "outputId": "0b1b1527-2adc-441a-b17a-0134938d780b"
   },
   "source": [
    "print(multi_tree.score(X_mt_train, y_mt_train))\n",
    "print(multi_tree.score(X_mt_test, y_mt_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17d3ff33",
   "metadata": {
    "id": "17d3ff33"
   },
   "source": [
    "## Gini Impurity\n",
    "To inform split decisions, `DecisionTreeClassifier()` **minimizes** gini impurity by default. Consider the case when you have $k$ possible classes. Then, the **gini impurity** of a given set of observations is\n",
    "$$\n",
    "G = 1 - \\sum_{i=1}^{k}p_i^2\n",
    "$$\n",
    "where\n",
    "$$p_i := \\frac{\\text{# of observations of class }k}{\\text{# of observations in set }}$$\n",
    "\n",
    "When all observations are of a single class, $G=0$ (think about why this is). If many classes share similar representation, the gini impurity will be closer to 1. The best split is the one that results in the two subsets of data that have the lowest **weighted** Gini Impurity. That is, each subset's gini impurity is weighted by the number of data points in that subset.\n",
    "\n",
    "## Entropy\n",
    "**Entropy** is incredibly similar but uses a slightly different functional form. Just like gini impurity, the best split when considering entropy is the one that results in the two subsets of data that have the lowest **weighted** entropy.\n",
    "$$\n",
    "E = -\\sum_{i=1}^{k}p_i \\log\\left(p_i\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93c109e",
   "metadata": {
    "id": "f93c109e"
   },
   "source": [
    "## Random Forest Classifier\n",
    "Random Forest Classifiers are basically the same as their regression counterpart. To predict a given observation, the forest sends the observation to each tree. Each tree finds the terminal node in which the given observation belongs and returns $p_i$ for each of the classes in that terminal node. Then, the forest finds the average $p_i$ across all classes and selects the class with the highest **average** $p_i$.\n",
    "\n",
    "Below, we benchmark the Random forest classifier against the multiclass logistic regression for the decile score prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "id": "4eed6420",
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 94
    },
    "id": "4eed6420",
    "outputId": "dc6fbe23-9b28-4258-9356-165c645c9625"
   },
   "source": [
    "# Initialize model\n",
    "rf_mclass = ensemble.RandomForestClassifier(n_estimators = 10000, max_depth = 20)\n",
    "rf_mclass.fit(X_m_train, y_m_train)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9dfaffbe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9dfaffbe",
    "outputId": "b67ca78a-6337-4080-e4f6-6aed5020197d"
   },
   "source": [
    "print(rf_mclass.score(X_m_train, y_m_train))\n",
    "print(rf_mclass.score(X_m_test, y_m_test))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "fv4iinBtMUYZ"
   },
   "id": "fv4iinBtMUYZ",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
